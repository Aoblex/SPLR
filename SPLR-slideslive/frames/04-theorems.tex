\section{Theorems}
\begin{frame}{Eigenvalue Structure}
    \begin{theorem}[Eigenvalue Guarantees]
    \label{thm:incremental-sparsification}
        $\forall \Omega \subseteq \bar{\Omega}: \exists k, \text{s.t.}(H_{\Omega})^k > 0$, $H_{\Omega}$ has the following properties:
        \begin{equation*}
            \begin{aligned}
                \lambda_{\max}(H_{\Omega}) &\leq \lambda_{\max}(H), \\
                \lambda_{\min}(H_{\Omega}) &\geq \lambda_{\min}(H), \\
            \end{aligned}   
        \end{equation*}
        where $H = H_{\bar{\Omega}} = H(x)$. The equalities hold if and only if $\Omega = \bar{\Omega}$.
    \end{theorem}

    Theorem \ref{thm:incremental-sparsification} shows that:
    \begin{itemize}
        \item Positive definiteness is maintained after sparsification
        \item The sparsified Hessian has a smaller condition number
        \item Such theorem allows for highly flexible algorithm designs
    \end{itemize}
\end{frame}

\begin{frame}{Convergence Analysis}
    \begin{theorem}[Global Convergence]
         \label{thm:global_convergence}
        Let $x_0$ be an arbitrary initial value, and $\{x_k\}$ be generated by the SPLR algorithm. Then
        \[
        \lim_{k\rightarrow\infty} \Vert g(x_k) \Vert = 0.
        \]   
    \end{theorem}
    
    \begin{theorem}[Linear Convergence]
    \label{thm:linear_convergence}
    Let $f^*$ be the optimal value of $f(x)$. Then for all $k\ge 1$, there is a constant $0<r<1$ such that
    \[
    f(x_{k+1})-f^*\le r[f(x_k)-f^*].
    \]
    \end{theorem}
\end{frame}